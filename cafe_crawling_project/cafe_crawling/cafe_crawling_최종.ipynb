{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c394ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime\n",
    "import time\n",
    "from urllib.parse import quote, unquote\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "def switch_frame(frame):\n",
    "    browser.switch_to.default_content()  # frame 초기화\n",
    "    browser.switch_to.frame(frame)  # frame 변경\n",
    "\n",
    "# 각 iframe들 [searchIframe / entryIframe / ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b13ace6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------1/6------------------------------------\n",
      "-------------------------------1번째-------------------------------\n",
      "-------------------------------2번째-------------------------------\n",
      "-------------------------------3번째-------------------------------\n",
      "-------------------------------4번째-------------------------------\n",
      "-------------------------------5번째-------------------------------\n",
      "-------------------------------6번째-------------------------------\n",
      "-------------------------------7번째-------------------------------\n",
      "-------------------------------8번째-------------------------------\n",
      "-------------------------------9번째-------------------------------\n",
      "-------------------------------10번째-------------------------------\n",
      "-------------------------------11번째-------------------------------\n",
      "-------------------------------12번째-------------------------------\n",
      "-------------------------------13번째-------------------------------\n",
      "-------------------------------14번째-------------------------------\n",
      "-------------------------------15번째-------------------------------\n",
      "-------------------------------16번째-------------------------------\n",
      "-------------------------------17번째-------------------------------\n",
      "-------------------------------18번째-------------------------------\n",
      "-------------------------------19번째-------------------------------\n",
      "-------------------------------20번째-------------------------------\n",
      "-------------------------------21번째-------------------------------\n",
      "-------------------------------22번째-------------------------------\n",
      "-------------------------------23번째-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NoSuchElementException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 54\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     browser\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div[3]/div/div[3]/div[1]/ul/li[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]/div[1]/a/div/div/span[1]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:93\u001b[0m, in \u001b[0;36mWebElement.click\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(Command\u001b[38;5;241m.\u001b[39mCLICK_ELEMENT)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:394\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m: Message: element click intercepted: Element <span class=\"TYaxT\">...</span> is not clickable at point (63, 756). Other element would receive the click: <path class=\"place_ad_label_border\" d=\"M30 18H9c-5 0-9-4-9-9s4-9 9-9h21c5 0 9 4 9 9s-4 9-9 9zM9 1C4.6 1 1 4.6 1 9s3.6 8 8 8h21c4.4 0 8-3.6 8-8s-3.6-8-8-8H9z\"></path>\n  (Session info: chrome=119.0.6045.160)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6C5F182B2+55298]\n\t(No symbol) [0x00007FF6C5E85E02]\n\t(No symbol) [0x00007FF6C5D405AB]\n\t(No symbol) [0x00007FF6C5D87A77]\n\t(No symbol) [0x00007FF6C5D85E39]\n\t(No symbol) [0x00007FF6C5D83C08]\n\t(No symbol) [0x00007FF6C5D82C8A]\n\t(No symbol) [0x00007FF6C5D787BF]\n\t(No symbol) [0x00007FF6C5DA20AA]\n\t(No symbol) [0x00007FF6C5D780CF]\n\t(No symbol) [0x00007FF6C5DA22C0]\n\t(No symbol) [0x00007FF6C5DBAAA4]\n\t(No symbol) [0x00007FF6C5DA1E83]\n\t(No symbol) [0x00007FF6C5D7670A]\n\t(No symbol) [0x00007FF6C5D77964]\n\tGetHandleVerifier [0x00007FF6C6290AAB+3694587]\n\tGetHandleVerifier [0x00007FF6C62E728E+4048862]\n\tGetHandleVerifier [0x00007FF6C62DF173+4015811]\n\tGetHandleVerifier [0x00007FF6C5FB47D6+695590]\n\t(No symbol) [0x00007FF6C5E90CE8]\n\t(No symbol) [0x00007FF6C5E8CF34]\n\t(No symbol) [0x00007FF6C5E8D062]\n\t(No symbol) [0x00007FF6C5E7D3A3]\n\tBaseThreadInitThunk [0x00007FFC0D067344+20]\n\tRtlUserThreadStart [0x00007FFC0DB026B1+33]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     browser\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div[3]/div/div[3]/div[1]/ul/li[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]/div[1]/a/div/div/span[1]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     56\u001b[0m     browser\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div[3]/div/div[4]/div[1]/ul/li[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]/div[1]/a/div/div/span[1]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# 카페정보를 받아오는 시간 충분하게 받아줌 인터넷이 느려 충분히 시간 줘야됌\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NoSuchElementException' is not defined"
     ]
    }
   ],
   "source": [
    "ls2 = ['강남구', '강동구', '강북구', '강서구', '관악구', '광진구', '구로구',' 금천구', '노원구', '도봉구', '동대문구', '동작구', '마포구',\n",
    "     '서대문구', '서초구', '성동구', '성북구', '송파구', '양천구', '영등포구', '용산구', '은평구', '종로구', '중구', '중랑구']\n",
    "# -----------------------------------25개의 구를 한번에 하면 인터넷 문제로 오류가 자주남-----------------------------------\n",
    "# -----------------------------------5개 구로 쪼개서 하나씩 돌리며 파일을 만듬-----------------------------------\n",
    "ls = ['강남구']\n",
    "\n",
    "cafe_name_list = []\n",
    "cafe_classification_list = []\n",
    "cafe_review_count_list = []\n",
    "cafe_address_list = []\n",
    "cafe_review_list = []\n",
    "background_image_url_list = []\n",
    "cafe_menu_ls_list = []\n",
    "\n",
    "# 처음 모든구를 한번에 돌릴려고 했으나 인터넷문제로 에러가 자주남. for문 제거\n",
    "# for i in range(len(ls)):  # 모든 자치구에 있는 카페를 알아보기 위해 반복\n",
    "# 브라우저 열기\n",
    "keyword = quote(f'{ls[i]} 카페')\n",
    "url = 'https://map.naver.com/p/search/' + keyword  \n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "# 스위치 변경\n",
    "switch_frame(\"searchIframe\")\n",
    "time.sleep(1)\n",
    "# 버튼 변수들\n",
    "next_btn = browser.find_elements(By.CSS_SELECTOR, '.zRM9F> a')\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "# 버튼길이만큼 반복 첫번재는 이전버튼 이므로 제외\n",
    "for btn in range(len(next_btn))[1:]: \n",
    "    print(f'------------------------------------{btn}/{len(next_btn)-1}------------------------------------')\n",
    "\n",
    "    #body부분을 잡기 위해 쓸데없는 버튼을 클릭해줌\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"_pcmap_list_scroll_container\"]/ul/li[1]/div[1]/div[1]/div').click()\n",
    "\n",
    "    #검색결과가 모두 보이지 않기 때문에 page down을 눌러 끝까지 펼쳐준다.\n",
    "    for scroll_1 in range(8):\n",
    "        browser.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "\n",
    "    resp = browser.page_source     # 처음 브라우저 소스와, 버튼을 누를때마다 그것에 관한 페이지 소스 가져옴\n",
    "    soup = BeautifulSoup(resp, 'html.parser') # 브라우저소스를 html로 변환하여 가져옴\n",
    "    length = len(soup.select('#_pcmap_list_scroll_container > ul > li'))+1  # 한페이지에 보이는 카페들의 개수 반환\n",
    "\n",
    "    # 카페들의 개수만큼 반복\n",
    "    for j in range(1, length+1):\n",
    "        # 스위치 변경\n",
    "        switch_frame(\"searchIframe\")\n",
    "\n",
    "        # 해당하는 카페에 정보를 알기위해 그 카페를 클릭해줌\n",
    "        # 구마다 /html/body/div[3]/div/div[3]....../sapn[1] or /html/body/div[3]/div/div[4]....../sapn[1] 가 있음\n",
    "        try:\n",
    "            browser.find_element(By.XPATH, f'/html/body/div[3]/div/div[3]/div[1]/ul/li[{j}]/div[1]/a/div/div/span[1]').click()\n",
    "        except NoSuchElementException:\n",
    "            browser.find_element(By.XPATH, f'/html/body/div[3]/div/div[4]/div[1]/ul/li[{j}]/div[1]/a/div/div/span[1]').click()\n",
    "        finally:\n",
    "            # 카페정보를 받아오는 시간 충분하게 받아줌 인터넷이 느려 충분히 시간 줘야됌\n",
    "            time.sleep(3)\n",
    "\n",
    "        # 스위치 변경 \n",
    "        switch_frame(\"entryIframe\")\n",
    "\n",
    "        # 버튼이 누를 필요 없어서 제거\n",
    "#---------------------------------------------------------------------------------------\n",
    "#             # 가끔 버튼을 찾지 못함\n",
    "#             try:\n",
    "#                 time.sleep(1)\n",
    "#                 browser.find_element(By.CLASS_NAME,'_UCia').click()\n",
    "#                 browser.implicitly_wait(10)\n",
    "#                 time.sleep(1)\n",
    "#             # 같은 버튼에 이름만 다른 클래스임\n",
    "#             except Exception as e:\n",
    "#                 browser.find_element(By.CLASS_NAME,'PkgBl').click()\n",
    "#                 browser.implicitly_wait(10)\n",
    "#                 time.sleep(1)\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "        print(f'-------------------------------{j}번째-------------------------------')\n",
    "\n",
    "        # 카페 주소 저장\n",
    "        cafe_addr = WebDriverWait(browser,1000).until(EC.presence_of_element_located((By.XPATH,'/html/body/div[3]/div/div/div/div[5]/div/div[2]/div[1]/div/div[1]/div/a/span[1]')))\n",
    "\n",
    "        cafe_address = cafe_addr.text\n",
    "\n",
    "\n",
    "        # 이미지 가져오기\n",
    "        background_image_style = browser.find_element(By.XPATH,'/html/body/div[3]/div/div/div/div[1]/div/div[1]/div/a/div').get_attribute('style')\n",
    "        background_image_url = re.search(r'url\\(\"([^\"]+)\"\\)', background_image_style).group(1)\n",
    "\n",
    "        # 위에서 버튼을 누르지 않으니 스크롤이 제대로 돌아가지 않음 entryIframe 내의 element 클릭 그래야 스크롤이 내려감\n",
    "        browser.find_element(By.ID, '_title').click()\n",
    "        for scroll in range(0,10):\n",
    "            browser.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "        resp = browser.page_source\n",
    "        soup = BeautifulSoup(resp, 'html.parser')\n",
    "\n",
    "        cafe_menu_ls = []\n",
    "        cafe_menu = soup.select('div.MN48z > div.erVoL > div.MENyI > span.VQvNX')\n",
    "        if len(cafe_menu) == 0:\n",
    "            cafe_menu = soup.select('div.YzCTi > div.Fi0vA > div.RhpMT > a.place_bluelink.ihmWt')\n",
    "            for menu in cafe_menu:\n",
    "                cafe_menu_ls.append(menu.text)\n",
    "        else:\n",
    "            for menu in cafe_menu:\n",
    "                cafe_menu_ls.append(menu.text)\n",
    "\n",
    "        # 리뷰 높은순 3개 가져오기.\n",
    "        cafe_review = []\n",
    "        try:\n",
    "            for k in range(1,4):\n",
    "                cafe_review.append(soup.select(f'div.place_section_content > div.iqjjt > a > ul > li:nth-child({k}) > div.CsBE9 > span.nWiXa')[0].text[1:-1])\n",
    "        except:\n",
    "            cafe_review.append('리뷰 참여자가 10명이 되지 않습니다.')\n",
    "\n",
    "        # 스위치 변경\n",
    "        switch_frame(\"searchIframe\")\n",
    "\n",
    "        resp = browser.page_source\n",
    "        soup = BeautifulSoup(resp, 'lxml')\n",
    "\n",
    "# --------------------------------------------------------평점이 없는 매장이 훨신 많아 평점 제거--------------------------------------------------\n",
    "        # 딕셔너리에 담을 변수들 생성\n",
    "        cafe_name = soup.select(f'#_pcmap_list_scroll_container > ul > li:nth-child({j}) > div.CHC5F > a.tzwk0 > div > div > span.TYaxT')[0].text\n",
    "        cafe_classification = soup.select(f'#_pcmap_list_scroll_container > ul > li:nth-child({j}) > div.CHC5F > a.tzwk0 > div > div > span.KCMnt')[0].text\n",
    "        cafe_review_count = soup.select(f'#_pcmap_list_scroll_container > ul > li:nth-child({j}) > div.CHC5F > div > div > span:nth-child(2)')[0].text\n",
    "        if cafe_review_count[:2] == '리뷰':\n",
    "            cafe_review_count = cafe_review_count[3:]\n",
    "        elif cafe_review_count[:2] != '리뷰':\n",
    "            try:\n",
    "                cafe_review_count = soup.select(f'#_pcmap_list_scroll_container > ul > li:nth-child({j}) > div.CHC5F > div > div > span:nth-child(3)')[0].text[3:]\n",
    "            except:\n",
    "\n",
    "                cafe_review_count = soup.select(f'#_pcmap_list_scroll_container > ul > li:nth-child({j}) > div.CHC5F > div > div > span:nth-child(1)')[0].text[3:]\n",
    "        else :\n",
    "            cafe_review_count = '0'\n",
    "\n",
    "        cafe_name_list.append(cafe_name)\n",
    "        cafe_classification_list.append(cafe_classification)\n",
    "        cafe_review_count_list.append(cafe_review_count)\n",
    "        cafe_address_list.append(cafe_address)\n",
    "        cafe_review_list.append(cafe_review)\n",
    "        background_image_url_list.append(background_image_url)\n",
    "        cafe_menu_ls_list.append(cafe_menu_ls)\n",
    "\n",
    "        dict_temp = {\n",
    "            'name':cafe_name_list,\n",
    "            'class':cafe_classification_list,\n",
    "            'review_count':cafe_review_count_list,\n",
    "            'address':cafe_address_list,\n",
    "            'review_top3':cafe_review_list,\n",
    "            'background_image_url':background_image_url_list,\n",
    "            'cafe_menu':cafe_menu_ls_list        \n",
    "        }    \n",
    "        \n",
    "    next_btn[-1].click() # 다음 버튼 클릭\n",
    "    time.sleep(5)\n",
    "    # 버튼이 눌리지 않으면 종료\n",
    "    if not next_btn[-1].is_enabled():\n",
    "        break\n",
    "\n",
    "browser.implicitly_wait(10)\n",
    "browser.close()\n",
    "\n",
    "df = pd.DataFrame(dict_temp)\n",
    "df.to_csv('C:/Users/user/crawling_project/crawling_강남구.csv',index = False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87329969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'C:/Users/user/crawling_project'\n",
    "\n",
    "# 파일 경로 패턴을 사용하여 여러 파일을 선택\n",
    "files = glob.glob(file_path + '/*구.csv')\n",
    "\n",
    "# 각 파일을 DataFrame으로 읽어와 리스트에 추가\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "\n",
    "# 모든 DataFrame을 합치기\n",
    "result = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 결과를 새로운 파일로 저장\n",
    "result.to_csv(file_path + '/crawling_서울시_자치구_카페_조사.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b81362f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                    0\n",
       "class                   0\n",
       "review_count            0\n",
       "address                 1\n",
       "review_top3             0\n",
       "background_image_url    0\n",
       "cafe_menu               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b9059f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name class review_count address                      review_top3  \\\n",
      "2076  파리바게뜨 구로구청점  베이커리         999+     NaN  ['빵이 맛있어요', '친절해요', '매장이 청결해요']   \n",
      "\n",
      "                                   background_image_url  \\\n",
      "2076  https://search.pstatic.net/common/?autoRotate=...   \n",
      "\n",
      "                                              cafe_menu  \n",
      "2076  ['러브초', '생딸기프레지에 (15cm)', '꾸안꾸 치즈 아이스', '[NEW]...  \n"
     ]
    }
   ],
   "source": [
    "nan_rows = result[result['address'].isna()]\n",
    "\n",
    "# 출력\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63097518",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu = []\n",
    "for i in range(len(df.address)):\n",
    "    if pd.notnull(df.address[i]):  # NaN이 아닌 경우에만 슬라이싱\n",
    "        gu.append(df.address[i][3:6])\n",
    "    else:\n",
    "        gu.append('')  # NaN인 경우 빈 문자열 또는 다른 값을 설정\n",
    "\n",
    "g = pd.DataFrame({'gu': gu})\n",
    "df = pd.concat([df, g], axis=1)\n",
    "\n",
    "df = df[['name', 'gu', 'address', 'class', 'cafe_menu', 'review_count', 'review_top3', 'background_image_url']]\n",
    "\n",
    "# save_path/filename.csv 입력\n",
    "df.to_csv('C:/Users/user/crawling_project/crawling_서울시_자치구_카페_조사_최종.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3626ae98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m==\u001b[39m NaN ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NaN' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"address\"== NaN ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49f434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
